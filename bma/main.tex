%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Large Colored Title Article
% LaTeX Template
% Version 1.0 (15/8/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{parts/layout}

\begin{document}

\input{parts/commands}
\begin{titlepage}
  \thispagestyle{empty}
  \maketitle % Print the title
\end{titlepage}

%\thispagestyle{plain}

\tableofcontents
%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\chapter{Abstract}
% The first character should be within \initial{}
\initial{H}\textbf{ere is some sample text to show the initial in the introductory paragraph of this template article. The color and line height of the initial can be modified in the preamble of this document.}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\chapter{Introduction}


%------------------------------------------------

\section{Section 1}


\begin{itemize}
  \item First item in a list 
  \item Second item in a list 
  \item Third item in a list
\end{itemize}


%------------------------------------------------

\section{Section 2}


%------------------------------------------------
\chapter{Artificial Intelligence}
\section{What is AI?}

This question has about as many answers as there are people studying artificial intelligence. Without getting too philosophical one could say that intelligence is defined by the efficiency of patterns used to solve problems in a complex environment and the ability to learn or generate new patterns by observing or testing.

Man-made programs to solve such problems have been around for decades and as algorithm efficiency and computational power increases they get ever more powerful.

An AI usually is directed towards one goal. This could mean solving a mathematical challenge most efficiently, finding patterns in huge amounts of data to categorize new sets of data or even to find smarter ways of learning.


\section{History of the Field}

\section{Technological Singularity}

\newpage
\section{Real World Applications}

Artificial Intelligence Technology 

\subsection{Amazon Recommendations}
Everyone knows the ``People who bought X also bought Y''-kind of recommendations online shops provide you with. Sometimes they are spot on and other times they are as far off as they can be. The idea of course is to find patterns in customer interest which can be used to offer people things they are likely to buy thereby increasing profit.

So how do they work? It is all about finding similarities in data. In the case of products you have attributes like cost, size, style, manufacturer, etc. that let classify the product. Yet taken for themselves they are close to useless because two products that are similar in size and cost might not have anything in common at all. It's even worse. Imagine two lamps that have almost the same price, the same size and the same style. What's the probability that someone who bought the one lamp would also want to buy the other? Close to zero. Now why is that? For one thing it's unlikely that someone needs two lamps. It's even less likely that they would need two almost identical lamps. So apart from flat stats about a product one needs to take into account the nature of the product to make good recommendations. An example of such would be matching light bulbs for the lamp. 
This tells us two things. First we need to categorize a product. Not only in it's use (e.g. lamp, chair, table, clock) but also in what frequency demand for it arises. So for example one would like to buy a new shirt every month whereas a new computer is only required every few years. The second thing is that there are certain items which depend on other items to function. A coffee machine needs filters, a toy car needs batteries and a lamp needs light bulbs. Hence to make a smart suggestion one would link those items one way so that buying a laptop would yield to recommendation of a mouse but buying cat food would not recommend acquiring a new feline companion.

This probably all sounds perfectly reasonable. Every halfway decent online shop should be capable handling categories and dependencies between products. Since the assignment of such seems more or less obvious even to an intern why not just hire some of those to do it? It doesn't require an expensive AI to figure out that it would be a good idea to recommend "Harry Potter" part two to someone who bought part one. Yet if those where to only clues to go on the list of suggestions would be pretty short for most products. To make matters worse: Since those types of relations are so obvious, it's likely the customer thought of them already so they don't yield as much additional sales as "smart" recommendations would. This is where the AI algorithms come in.

Imagine what a resale company could do if they understood their customer's taste. If they knew what style of clothing she likes, whether she lives near a beach and thus is more likely to buy a new bathing suit in spring or when her mothers birthday was so they could recommend gifts for older women at that time of year. Admittedly these examples might are little extreme yet the efforts of the so called data miners definitely go in that direction. These three examples have been picked for a reason because they each depend on different types of data.

The style for example can be derived from the clothes the customer has bought in the past. From those the retailer could categorize his customers into groups that appear to like the same products and recommend things person A from this group bought to person B. Of course there is more to it than that. For example one needs to filter out noise which may be a piece of clothing a customer bought for someone else and thus does not match the pattern. Now an especially smart AI could even make a connection between such purchases and the friend or family member they where meant to. This data then could be used for a variety of further predictions about the customers buying behavior.

The fact whether someone lives near a beach (or swimming pool for that matter) can be easily determined by matching the IP address from which requests to the store website were made to the corresponding location. There is a lot more of this type of information that one can easily read from the web request like browser type, mobile phone model, referring website and by matters of recurring visits from different locations even estimates about the customer's place of work or their boyfriend's address. Now this type of information is only as useful as one is able to make sense of it. This is the true strength of an AI because it can find patterns in data that no one has been looking for so far.

\subsection{Web Search Algorithms}

In web search it is even more important than with large selections of products to have an automated way to make precise estimations about the value of something to someone. Furthermore the search provider not only wants to establish a prediction about how likely a person is to be looking for a certain web page but also about how likely he or she is to be looking for it right now. A lot of things factor into this like previously visited websites, behavior in choosing links and probably over a thousand other data points that might have been collected about a person. Yet even if there is no previous information about a search engine user available they can still make predictions based on the location, language and outside factors. An example of this might be the following scenario:

A user sits down in a Boston coffee shop using a browser, the signature of which has been modified to prevent it from being tracked back to that certain individual, to access her search provider of choice. She types in the word "dolphins". Now the first assumptions is that she is looking for the marine mammals. Yet the night before the football team by the same name might have played which would vastly improve the relevance of last nights game results. This increase of relevance can be justified by the fact that the user has set her language preference to American English which makes it far more likely that she is interested in the score than someone with a preference for French would be. But this isn't everything that is relevant to the search. Assuming that a band called "The Dolphins" is playing this week in Boston would make the band's website and a ticket sales site a lot more relevant to this search then it would be for someone from San Francisco.

Again this example shows how a shift in relevance might be obvious if pointed out but is extremely hard to predict for a human. Artificial intelligence can look at changes in user behavior (like increased requests to "The Dolphins" band website from Boston) and smartly adjust the ranking of certain websites. Of course this is an oversimplification of the problem but it shows nicely the basic idea of using AI to find patterns in changing data.

\subsection{DARPA Challenges}

The Defense Advanced Research Projects Agency has funded prize competitions that reward advances in research fields relevant to the Department of Defense. The initial challenge was to build autonomous ground vehicles or "self-driving cars". It was called the DARPA Grand Challenge and was first held in 2004. The goal was to build an autonomous vehicle that could drive the 240 km route through the desert fastest. None of the robot vehicles that entered the contest was able to complete the challenge. The one that performed best only finished five percent of the route before getting hung up on a rock. Therefore the event was repeated in 2005.

The second time around five vehicles were able to finish the race. With the team from Stanford University finishing first. 

In 2007 the DARPA Urban Challenge has been held requiring the vehicles to finish a 96 km course in an urban area while obeying traffic regulations and interacting with human drivers.

Currently Google is developing a driverless car which has already driven 300'000 miles without an accident. The project is led by Sebastian Thrun who also was the head of the Stanford team that won the 2005 DARPA Grand Challenge.

\subsection{Deep Blue}

In 1997 the IBM chess computer Deep Blue famously defeated the world champion Garry Kasparov. 

\subsection{IBM Watson}

\section{Traditional AI vs Neural Networks}

\section{Classification of AI problems}

There are four major attributes to an AI problem. Each of them has severe impact on how one approaches to solve that given problem.
\begin{description}
\item Firstly there is the question of observability. Some environments are {\bf fully observable} while others are only {\bf partially observable}.
\item Secondly there is the factor of randomness. An environment can be completely {\bf deterministic} which means that taking an action always result in the expected state or {\bf stochastic}. In the latter case there is chance involved in determining the outcome of an action. A good example for this would be a game that requires one to roll dice.
\item Thirdly a problem might either be {\bf discrete} or {\bf continuous}. The former one meaning that there is a finite number of things to do and things to observe and the latter meaning that there is a factor to the problem that is infinite and has to be handled accordingly.
\item Fourthly the problems environment can be {\bf benign} or {\bf adversarial} which would mean that there are factors to it that are actively contradicting the AI's objective by taking actions that prevent it from achieving its goals. Most games are of adversarial nature because in order to succeed each player needs to take actions that make it harder for his opponents to win.
\end{description}

\newpage
\section{Important AI Algorithms in Detail}
\subsection{Tree Search}

In artificial intelligence many problems can be represented as trees. The first node of that tree is the initial state of the problem. From that state different actions can be taken which lead to new states. A good example of this is the game of Tic Tac Toe.

\begin{figure}[H]
  \includegraphics[width=0.45\textwidth]{images/tree_search_tic_tac_toe.png}
  \caption[\url{http://en.wikipedia.org/wiki/Tic-tac-toe}]{Tree representation of Tic Tac Toe turns from Wikipedia}
\end{figure}

There are different approaches for searching through trees each with their respective advantages and problems. Without additional knowledge about the situation the choice is between breadth-first search and depth-first search. The former one expanding always on the least explored path (broad) whereas the latter always expands the best explored (deepest) path. 

At the example of Tic Tac Toe this would mean that breadth-first search would look at all the nine first turn moves, evaluating whether they result in a winning state (which of course none of them do). Next it would look at the eight resulting states the opponent's turn could have put the game in for all of those nine states. This this result in another seventy-two nodes being expanded without any of them possibly being a winning state. The first winning state could occur in turn five which means that 26'132'625 states need to be calculated and evaluated. Yet since wins in the fifth turn are only possible if the opponent is playing badly on purpose it is far more likely that the tree has to be calculated to the 7th or even ninth turn.

Now depth-first search on the other hand would just pick one first turn move and expand on one of the possibly resulting second turn moves. This process is repeated until a final state is reached, which means that there are no actions that can be taken to transition this state into another. Final states in Tic Tac Toe are wins, loses and ties. After depth-first search reached a final state it backs up to the furthest expanded node and takes another action from that state.

In a game neither of those search methods is preferable because due to the adversarial nature of the game one always has to expand every possible action the opponent could take from a state in order to determine whether that state is "good" or "bad". If there is additional information available (like rules to this game) the AI can be taught smarter tree search algorithms. For example one could teach it to see certain patterns in game states which indicate that a state is preferable or not. A depth-first search that chooses nodes to expand on an estimated value of the resulting state instead of doing so randomly is called the A* algorithm.

\subsection{Bayes' Networks}

Often the problem an AI faces is stochastic and randomness suddenly plays a big role in choosing smart actions. Using probability to make decisions is a rather simple process when all the numbers are available. It does not take artificial intelligence to figure out that an action with a 90 percent chance of success is to be preferred to one with just 40 percent. But if there are multiple events whose chance of occurring depend on each other and not all of them are observable it gets trickier.

As an example of such a problem we assume a machine that can tell us the whether there will be an earthquake today or not. But this machine is faulty and in one out of six cases it lies. Now we know from statistics 0.03 is the probability of an earthquake happening in this area. Now given that the machine tells us today is an earthquake day what is the chance of an actual earthquake happening?

To calculate this one uses {\bf Bayes' Rule} which states this. 

\[P(A|B)=\frac{P(B|A) * P(A)}{P(B)}\]

This reads as: The probability of event A occurring given that we observed event B has occurred is equal to the probability of event B occurring given that event A occurred multiplied by the probability of event A occurring and normalized by the probability of event B occurring.

If event B is not known but is only dependent on event A, the rule can also be written as:

\[P(A|B)=\frac{P(B|A) * P(A)}{P(B|A)*P(A)+P(B|!A)*P(!A)}\]

Using this rule one can construct complicated networks of event that depend on each other thus making probabilities available that were previously unknown or are changed by readings of sensors.

\subsection{Markov Decision Processes}



\subsection{Natural Language Learning}
\subsection{Particle Filters}
\subsection{Heuristics}
\subsection{Game Theory}

\chapter{Project}
\input{parts/project}

%----------------------------------------------------------------------------------------

\chapter{Sum up}


\chapter{Sources and Glossary}

\section{Sources}
\begin{description}
  \item[Physics Engines] \hfill \\
    \begin{itemize}
      \item \printhref{http://en.wikipedia.org/wiki/Physics\_engine}{Physics engine - Wikipedia}
    \end{itemize}
  \item[Game Engines] \hfill \\
    \begin{itemize}
      \item \printhref{http://en.wikipedia.org/wiki/Game\_engine}{Game engine - Wikipedia}
      \item \printhref{http://www.crytek.com/cryengine}{CryENGINE $|$ Crytek}
      \item \printhref{http://www.unrealengine.com}{Game Engine Technology by Unreal}
      \item \printhref{http://en.wikipedia.org/wiki/Anvil\_(game\_engine)}{Anvil (game engine) - Wikipedia}
      \item \printhref{http://en.wikipedia.org/wiki/IW\_engine)}{IW engine - Wikipedia}
    \end{itemize}
  \item[The Game] \hfill \\
    \begin{itemize}
      \item \printhref{http://craftyjs.com/}{Crafty - Javascript Game Engine}
    \end{itemize}
\end{description}

\printglossary%[type=main,title={Glossary},toctitle={Glossary}]
\addcontentsline{toc}{section}{Glossary}

\section{Image Sources}
\listoffigures

\end{document}
